{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sequential.dataset_seq import Sequence, SequenceLoader\n",
    "from feature_extractor import feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_dir = os.path.join(\".\", \"mock data\")\n",
    "root_dir = os.path.join(\".\", \"mock data\", \"actions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIVITIES = {\n",
    "    \"take leg\": 0,\n",
    "    \"assemble leg\": 1,\n",
    "    \"grab drill\": 2,\n",
    "    \"use drill\": 3,\n",
    "    \"drop drill\": 4,\n",
    "    \"take screw driver\": 5,\n",
    "    \"use screw driver\": 6,\n",
    "    \"drop screw driver\": 7\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all sequences\n",
    "seqs = []\n",
    "for activity in os.listdir(root_dir):\n",
    "    folders = [folder for folder in os.listdir(os.path.join(root_dir, activity))\\\n",
    "               if os.path.isdir(os.path.join(root_dir, activity, folder))]\n",
    "    for folder in folders:\n",
    "        path = os.path.join(root_dir, activity, folder)\n",
    "        seq = Sequence(path, ACTIVITIES[activity], walk=True)\n",
    "        seqs.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_loader = SequenceLoader(seqs, feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data loading\n",
    "# for feature_vectors, activity in dataloader:\n",
    "#     print(feature_vectors.shape)\n",
    "#     print(activity)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sequential.model import LSTMActionClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n"
     ]
    }
   ],
   "source": [
    "feature_vector_size = 1000\n",
    "hidden_size = 512\n",
    "batch_size = 8\n",
    "num_frames = 3 # equals sequence length\n",
    "\n",
    "dataloader = DataLoader(seq_loader, batch_size=batch_size, shuffle=True)\n",
    "print(len(dataloader))\n",
    "\n",
    "num_classes = 8\n",
    "model = LSTMActionClassifier(feature_vector_size,\n",
    "                             num_frames,\n",
    "                             hidden_size,\n",
    "                             num_classes)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/105 [00:00<?, ?it/s]c:\\Users\\Marc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "100%|██████████| 105/105 [16:06<00:00,  9.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 2.0826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [15:42<00:00,  8.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 2.0730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [15:51<00:00,  9.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 2.0717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [15:26<00:00,  8.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 2.0701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [15:23<00:00,  8.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 2.0704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [15:15<00:00,  8.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 2.0702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [15:12<00:00,  8.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 2.0695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [15:57<00:00,  9.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 2.0697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [15:29<00:00,  8.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Loss: 2.0687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [15:51<00:00,  9.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 2.0680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    # x = feature vectors\n",
    "    # y = activity\n",
    "    for i, (x, y) in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        y = y.to(device)\n",
    "        x = x.to(device).float() # edit: x is no of shape [8, 3, 1000]\n",
    "        activity_oh = F.one_hot(y, num_classes).squeeze(0).float()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = model(x)\n",
    "        loss = criterion(out, activity_oh)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "        running_loss += loss.item()\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor([0, 6, 1, 3, 4, 1, 6, 6], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for _ in range(1):\n",
    "        x, y = next(iter(dataloader))\n",
    "        y = y.to(device)\n",
    "        x = x.to(device).float()\n",
    "        x_hats = model(x)\n",
    "        for x_hat in x_hats:\n",
    "            print(torch.argmax(x_hat))\n",
    "        print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"seq_model_params.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
